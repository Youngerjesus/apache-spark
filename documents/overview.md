아파치 스파크는 라이브러리들을 이용해 클라스터 환경에서 데이터를 병렬로 처리하는 컴퓨팅 엔진이다.

- 파이썬, 자바, 스칼라, R 을 통해서 개발이 가능하고
- SQL 이나 여러 머신러닝 라이브러리를 지원한다.
- 컴퓨팅엔진은 단일 머신부터 시작해서 수천대의 서버로 구성된 클러스터에서까지 활용이 가능하다.

스파크는 저장소에 의존하지도 않고 영구 저장소 역할을 수행하지도 않는다. 오로지 컴퓨팅 엔진으로만 제한을 해왔다.

- 그래서 Azure Storage, Amazon S3, Apache Hadoop, Apache Cassandra, Apache Kafka 등의 저장소를 지원한다.

### 스파크의 등장 배경

- 기존에 하드웨어 프로세서의 성능 개선이 많았던 시절에서는 이 부분에 초점이 맞춰졌었다.
- 근데 2005 년 이후로 하드웨어의 성능 자체의 개선 보다는 CPU 코어의 개수를 늘리는 쪽으로 발전이 되었다.
- 그래서 어플리케이션에서도 성능 향상을 위해서는 병렬로 프로그래밍을 생각하는 모델이 등장했고 이게 스파크의 기원(?) 이다.

## 스파크 간단하게 살펴보기

- 스파크는 한 대의 컴퓨팅 머신을 이용하는게 아니라 여러 컴퓨터의 자원을 모아서 하나의 컴퓨터 처럼 사용하도록 한다.
- 이렇게 하기 위해서는 클러스터에서 작업을 조율할 수 있도록 해야한다.
  - 스파크가 연산에 사용하는 클러스터는 Spark Standalone 클러스터 매니저, 하둡 YARN, Mesos 같은 클러스터 매니저에서 관리한다.
  - 사용자는 클러스터 매니저에 애플리케이션을 submit 하기만 하면 클러스터 매니저는 자원을 클러스터에게 할당해서 병렬 처리를 한다.

### 스파크 어플리케이션

- 스파크 어플리케이션은 Driver 프로세스와 Executor 프로세스로 구성된다.
  - 이 두 프로세스도 클러스터 매니저에 의해서 실행된다.
- Driver 프로세스는 클라스터의 노드 중 하나이며 main() 함수를 실행하는 역할을 하고, 스파크 어플리케이션의 정보 유지 관리, 사용자 프로그램 입력과 응답, Executor 프로세스의 작업과 관련된 분석, 배포, 그리고 스케쥴링 역할을 한다.
- Executor 프로세스는 드라이버 프로세스가 할당한 작업을 수행한다. 드라이버가 할당한 코드를 실행하고 다시 드라이버에게 보고하는 두 가지 역할을 수행한다.
- 사용자는 어플리케이션을 실행할 때 Python 과 R 같은 언어로도 작성할 수 있는데 결국은 SparkSession 객체를 통해서 실행할 수 있는 Bytecode 로 변환해주기 때문에 사용이 가능하다. Spark 를 실행하는 첫 진입점은 SparkSession 을 통해서 이뤄진다. (SparkSession 을 드라이버 프로세서 라고 봐도 된다.)

### 스파크 API

- 스파크에서는 Unstructured API (저수준의 비구조적 API) 와 Structured API (고수준의 구조적 API) 를 통해서 개발이 가능하다.
  - Unstructured 는 내가 기능을 한땀한땀 만들기 위해서 지원하는 API 같은 거라면
  - Structured 는 지원하는 기능이 추상화 되어있어서 만들어져있는 것.
  
### SparkSession

- 스파크 어플리케이션은 SparkSession 이라 불리는 드라이버 프로세스로 제어한다.
- 하나의 SparkSession 은 하나의 스파크 어플리케이션과 대응된다.

### DataFrame 

- 구조적 API 이다.
- 테이블의 데이터를 로우와 칼럼으로 표현할 수 있고 스키마를 가진다.

### 파티션 (Partition)

- 스파크는 Executor 프로세스가 병렬로 작업을 처리할 수 있도록 파티션이라 불리는 청크 단위로 데이터를 분할한다.
- 클러스터의 물리 머신에 존재하는 데이터 로우의 집합이라고 생각하면 된다.
- 즉 파티션이 1 이라면 수천 개의 Executor 가 있어도 병렬성이 1 이라는 것.
- DataFrame 을 사용한다면 파티션을 개별적으로 처리할 필요가 없다.
- 그리고 RDD 인터페이스를 이용하는 저수준 API 에서도 파티션이 제공된다.

### 트랜스포메이션 (Transformation)

- DataFrame 을 변경할려면 변경 방법을 스파크에 알려줘야 한다. 이 작업을 트랜스포메이션 이라고 한다.
- 기본적으로 데이터는 Immutable 하다. 즉 데이터를 한번 생성하면 변경할 수 없다. 그럼 어떻게 데이터를 변경해서 사용할 수 있느냐? 트랜스포메이션을 통해서만 가능하다.
- 그리고 스파크에서 실제 연산을 수행하라는 Action 을 지정하지 않는다면 트랜스포메이션은 일어나지 않는다.
- 트랜스포메이션은 두 가지 유형이 있다. Narrow Dependency 와 Wide Dependency.
- Narrow Dependency 는 각 입력 파티션이 하나의 출력 파티션에만 영향을 미친다.
- Wide Dependency 는 하나의 입력 파티션이 여러 출력  파티션에 영향을 미친다.

### 지연 연산 (Lazy Evaluation)

- 지연 연산은 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식을 의미한다.
- 최적화를 위해 스파크는 연산 명령이 내려진 즉시 데이터를 수정하지 않고 코드를 수정하는 마지막까지 기다린 다음에 실행 계획을 세운 후 컴파일 한다.
- explain() 을 통해서 실행 계획을 볼 수 있다.

### 액션 (Action)

- 트랜스포메이션들을 통해서 실행 계획을 세우더라도 실제 액션이 없다면 스파크는 작업을 처리하지 않는다.
- 가장 간단한 action API 는 collect(), count() 같은 것들이 있다.
- 액션을 지정하면 스파크 Job 이 시작된다.

### 스파크 UI

- 스파크 UI 는 스파크 잡의 진행 상황을 모니터링할 때 사용한다.
- 로컬 모드에서 스파크를 실행했다면 스파크 UI 의 주소는 [localhost:4040](http://localhost:4040/) 에서 볼 수 있다.
- 스파크 UI 를 통해 스파크 잡을 튜닝하고 디버깅할 수 있다.

### 운영용 어플리케이션 실행하기

- spark-submit 명령을 사용하면 개발한 프로그램을 운영용 어플리케이션으로 쉽게 전환하는게 가능하다.
- spark-submit 을 통해서 애플리케이션 코드를 클러스터에 전송해서 실행시키는 역할을 하기 때문이다.

### Dataset: 구조적 API

- Dataset 은 자바나 스칼라와 같이 정적 타입을 지원하기 위한 스파크의 구조적 API 이다.
- DataFrame 과 비슷한데 DataFrame 은 테이블형 데이터를 보관할 수 있는 Row 타입의 객체로 구성되었다.
- Dataset 은 여기에다가 클래스에 할당할 수 있도록 해서 타입 안정성을 지원했다. 그래서 컴파일타임의 에러를 발견할 수 있다.

### 구조적 스트리밍

- 구조적 스트리밍은 스트림 처리용 고수준 API 를 말한다.
- 이를 이용하면 배치 모드의 연산을 스트리밍 방식으로 실행할 수 있고 지연 시간을 줄이고 증분 처리를 할 수 있다.
- 이 API 의 장점은 배치 처리용 코드를 일부 수정하면 스트리밍 처리를 위한 값을 빠르게 얻을 수 있다는 장점이 있다.
  - read() 대신에 readStream() 으로만 바꾸면 된다던지.
- 스트리밍 API 도 지연 연산 (Lazy Operation) 이므로 실행하기 위해서는 스트리밍 액션을 호출해야 한다.
- 스트리밍 액션의 경우에는 데이터를 어딘가에 채워 넣어야 하므로 일반적인 count() 와 같은 정적 액션과는 조금 다르다.

### 머신러닝과 고급 분석

- 스파크가 인기 있는 또 다른 이유는 머신러닝 알고리즘 라이브러리인 MLlib 를 사용할 수 있다는 것이다.
- MLlib 를 이용하면 대용량 데이터를 대상으로 다음과 같은 작업을 하는게 가능하다.
  - preprocessing
  - munging
  - model training
  - prediction
    - classification
    - regression
    - clustrering
    - deep learning

### 저수준 API

- 스파크는 RDD 를 통해서 자바나 파이썬 객체를 다루는 필요한 기본 기능들을 제공해준다.
- 그리고 스파크의 모든 기능들이 대부분 RDD 를 기반으로 만들어졌다.