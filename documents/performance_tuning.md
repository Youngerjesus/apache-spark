# 성능 튜닝 

여러 가지 이유로 잡을 더 빠르고 효율적으로 실행해야 하는 경우가 있다. 그러므로 이 장에서는 잡의 실행 속도를 높이기 위한 성능 튜닝 방법을 알아보자.

모니터링 방식과 마찬가지로 성능을 튜닝할 수 있는 다양한 방식이 존재한다. 예를 들어 네트워크가 엄청나게 빠른 환경이라면 가장 큰 비용이 드는 셔플 처리 시간을 줄여서 스파크 잡을 보다 더 빠르게 처리할 수 있다.

하지만 이러한 환경을 갖추는 것 자체는 쉽지 않다. 그러므로 코드나 설정 변경을 통한 성능 제어 방법을 알아야 한다.

사용자는 스파크 잡의 모든 단계를 최적화 하고 싶을 것이다. 그 중 주요 영역을 꼽아보면 다음과 같다.

- 코드 수준의 설계 (RDD 나 DataFrame)
- 보관용 데이터
- 조인
- 집계
- 데이터 전송
- 애플리케이션별 속성
- 익스큐터 프로세스의 JVM
- 워커 노드
- 클러스터와 배포 환경

여기서는 앞서 나열한 항목 위주로 알아보겠다.

스파크 잡의 성능 튜닝 방법은 크게 두 가지가 있다.

첫째로는 속성값을 설정하거나 런타임 환경을 변경해서 간접적으로 성능을 높이는 방법.

- 환경 변화를 통한 (메모리 추가 등) 성능 개선인가? 똑같은 코드인데.

둘째로는 개별 스파크 잡, 스테이지, 테스크 성능 투닝, 코드 설계를 변경해서 직접적으로 성능을 높이는 방법이다.

잡의 성능 향상을 확인하는 데 가장 좋은 방법 중 하나는 좋은 모니터링 도구와 잡 이력 추적 환경을 구성하는 것이다.

이력 정보가 없으면 잡의 성능이 실제로 향상되었는지를 알기 어렵기 떄문에.

## 간접적인 성능 향상 기법

여기서는 하드웨어 개선과 같은 너무 뻔한 사항은 제외하고 사용자가 제어 가능한 사항을 중심으로 집중적으로 알아보겠다.

### 스칼라, 자바, 파이썬, R

구조적 API 로 만들 수 없는 트랜스포메이션을 사용해야 한다면, 즉 RDD 나 UDF 를 사용해야 한다면 R 과 파이썬은 사용하지 않는게 좋다.

UDF 는 데이터를 JVM 객체로 변환하는 작업이 필요하다 (자바, 스칼라를 사용하는 경우는 별 문제가 없을 수 있다.) 그래서 파이썬이나 R 은 적합하지는 않다. 구조적 API 만 사용하는 경우라면 이것도 문제 없다.

파이썬에서 RDD 를 실행한다면 파이썬 프로세스와 JVM 사이에 데이터 직렬화 역직렬화 비용이 매우 크기 때문에.

### DataFrame, SQL, Dataset, RDD

모든 언어에서의 DataFrame, DataSet, SQL 속도는 모두 동일하다.

RDD 만이 다르다. 그러므로 RDD 를 사용할거면 스칼라나 자바를 무조건 쓰도록 하자.

RDD 를 그냥 쓰는 것보다도 DataFrame, Dataset, SQL 을 쓰도록 하자. 스파크의 최적화 엔진이 더 나은 RDD 를 작성하도록 해준다.

### RDD 객체 직렬화

RDD 를 사용할 경우 직렬화 라이브러리에 Kryo 를 무조건 쓰도록 하자.

Kryo 가 가장 효율적이다. 이를 사용할거면 `spark.serializer` 속성 값을 `org.apache.spark.serializer.KryoSerializer` 로 설정하면 된다. 그리고 사용하는 클래스를 ``spark.kryo.classesToRegister` 속성값에 지정해서 Kryo Serializer 에 등록해야 한다.

### 보관용 데이터

자주 발생하지는 않지만 조직의 동료들이 여러 가지 분석을 수행하기 위해서 동일한 데이터셋을 여러번 읽을 수도 있다.

그러므로 이를 효율적으로 읽어들이기 위해서 저장해야한다. 적절한 저장소 시스템과 데이터 포맷을 선택하자.

그리고 데이터 파티셔닝이 가능한 데이터 포맷을 사용하자.

다음으로 이를 위한 여러가지 기준들을 설명하겠다.

**파일 기반 장기 데이터 저장소**

파일 기반으로 데이터를 저장하는 경우 CSV 파일, blob 파일부터 아파치 파케이 처럼 정교한 포맷에 이르기까지 다양한 데이터 포맷을 사용할 수 있다.

CSV 파일 같은 경우는 구조화되어 있는 것처럼 보이지만 파싱 속도가 매우 느리고 부적절하게 처리된 개행 문자로 인해 예외 상황이 자주 발생한다. 그러므로 대량의 파일에는 적합하지 않다.

가장 효율적인 포맷으로는 아파치 파케이가 있다.

파케이는 데이터를 바이너리 파일에 칼럼 지향 방식으로 저장할 수 있고 쿼리에서 사용하지 않는 데이터는 빠르게 건너 뛸 수 있도록 몇 가지 통계를 함께 저장한다.

스파크는 파케이 데이터소스를 내장하고 있으니 파케이 파일과 잘 호환된다.

**분할 가능한 파일 포맷과 압축**

어떤 파일 포맷을 선택하더라고 분할 가능한 포맷인지 확인하자.

분할 가능한 포맷을 사용하면 여러 테스크가 병렬적으로 서로 다른 부분을 읽는게 가능하다.

JSON 처럼 분할이 불가능한 파일 포맷을 사용하면 단일 머신에서 전체 데이터를 읽어야 하니까 병렬성이 급격하게 떨어진다.

그리고 압축 포맷은 분할 가능 여부를 결정하는 주요 요소다.

ZIP 파일이나 TAR 압축 파일은 분할할 수 없다. 즉 병렬로 읽을 수 없다.

하둡이나 스파크 같은 병렬 처리 프레임워크는 gzip, bzip 또는 lz4 를 이용해 압축된 파일이라면 분할할 수 있으므로 이렇게 분할 가능한 포맷으로 만들자.

**테이블 파티셔닝**

테이블 파티셔닝은 데이터를 날짜 같은 특정 필드 값을 기준으로 디렉터리를 구분해서 저장하도록 하는 방법이다.

이를 이용하는 이유는 특정 범위의 데이터가 필요할 때 필요 없는 데이터를 검색하지 않도록 Skip 하기 위해서 사용한다.

스파크 역시 다양한 데이터 소스에서 테이블 파티셔닝 기능을 지원한다.

아파치 하이브 같은 저장소 매니저는 테이블 파티셔닝을 지원한다.

쿼리에서 `date` 칼럼이나 `customerId` 칼럼을 기준으로 자주 필터링을 한다면 그 칼럼을 기준으로 테이블 파티셔닝을 하자.

파티셔닝을 하면 쿼리에서 읽어야 하는 데이터양을 크게 줄일 수 있어서 쿼리를 훨씬 빠르게 처리할 수 있다.

**버켓팅**

데이터를 버켓팅하면 스파크는 사용자가 조인이나 집계를 수행할 때 미리 데이터를 사전 파티셔닝 (pre-partition) 할 수 있다.

즉 버켓팅을 사용하면 조인 칼럼의 값에 따라 데이터를 미리 분할할 수 있으므로 셔플을 방지해서 데이터 접근 속도를 높일 수 있다.

버켓팅은 물리적 데이터 분할 방법의 보완제로서 보통 파티셔닝과 함께 사용한다.

**파일 수**

데이터를 파티션이나 버켓으로 구성할 때 파일 수와 저장할려는 파일 크기도 고려를 해야한다.

작은 파일이 많을 경우 부하가 발생하기도 한다.

그 이유로는 HDFS 에서 데이터 블록의 사이즈는 최대 128MB 로 관리한다.

5MB 파일의 크기가 30 개가 있다면 총 150MB 가 되서 2 개의 블록이 필요하다고 생각할 수 있지만 총 30 개의 블록에다가 저장한다.

물론 파일의 크기가 크다면 I/O 작업이 줄지만 테스크 수행 시간은 더욱 길어질 수 있다. 그리고 파일의 수가 많다면 그 만큼 병렬성을 더할 수도 있다.

이런 상황에서 최적으로 사용하려면 최수 수십 메가 바이트의 데이터를 갖도록 파일의 크기를 조정하는 것이 가장 좋다.

데이터 쓰기 작업을 할 때 스파크 2.2 버전에 추가된 옵션을 사용해서 파티션을 제어할 수 있다.

maxRecordsFile 옵션을 쓸 때 파일당 저장하는 레코드 수를 지정할 수 있다.

### 파티션 설정

파티션 수는 모든 스파크 잡에서 중요하다.

파티션 수가 너무 적으면 소수의 노드만 작업을 수행하기 때문에 데이터 치우침이 발생하고 파티션 수가 너무 많으면 테스크가 너무 많아지므로 부하가 생긴다.

### 메모리 부족과 가비지 컬렉션

스파크 잡 실행 과정중에 익스큐터나 드라이버 머신의 메모리가 부족해서 memory pressure 로 인해 테스크를 완료하지 못할 수가 있다.

이 원인에는 애플리케이션 실행 중에 너무 많은 메모리를 사용한 경우가 있고, JVM 내에서 객체가 너무 많이 생성되서 GC 로 인해 실행 속도가 느려지는 경우 때문이다.

이 객체 생성 문제는 구조적 API 를 사용하면 된다.

구조적 API 는 JVM 객체를 전혀 생성하지 않으니까 문제가 없다.

스파크 공식 문서에서는 RDD 와 UDF 기반의 어플리케이션을 위한 몇 가지 좋은 가비지 컬렉션 튜닝 지점을 설명하고 있다.

가비지 컬렉션의 튜닝의 첫 단계는 먼저 해결해야할 부분을 찾기 위해서 통계 데이터를 모우는 것부터 시작이다.

`spark.executor.extraJavaOptions` 속성에 스파크 JVM 옵션으로 `-verbose:gc` , `-XX+PrintGCDetails` , `-XX:+PrintGCTimeStamps` 값을 추가해서 통계를 모을 수 있다.

이렇게 속성 값을 설정한 다면 가비지 컬렉션이 발생할 때마다 로그에 메시지가 출력된다. 로그는 드라이버가 아닌 워커 노드의 stdout 파일에 저장된다.

가비지 컬렉션 튜닝의 목표는 수명이 긴 캐시 데이터를 Old Generation 영역에서 보관하도록 하고 수명이 짧은 객체를 Old Generation 으로 보관하지 않도록 Young Generation 에 충분한 영역을 보장하는 것이다.

결국에는 Full GC 를 줄이는게 목표다.

만약 Full GC 가 자주 발생한다면 캐싱에 사용하는 데이터의 메모리를 줄이도록 해보자. (``spark.memory.fraction``)

만약 Minor GC 는 자주 발생하는데 Full GC 는 자주 발생하지 않는다면 에덴 영역의 데이터를 늘려서 실행하는 중간에 GC 를 줄여서 Full GC 로 데이터가 넘어가지 않도록 해줄 수 있다.

예로 Eden 영역의 메모리 크기가 E 라면 Yonug Generation 영역의 크기를 `-Xmn=4/3*E`  로 설정할 수 있다.

사용자 테스크가 HDFS 에서 데이터를 읽는다면 HDFS 에서 읽어들이는 데이터 블록 크기로 추정할 수도 있다. 압축이 해제된 블록의 크기는 보통 원본 블록의 2~3 배가 된다.

따라서 3~4 개의 테스크를 이용해 블록 파일 크기가 128MB 인 HDFS 파일을 읽는다면 Eden 영역의 메모리 크기는 4*3*128MB 로 약 1.5GB 로 추정해볼 수 있다.

또 `-XX:+UseG1GC` 를 이용해 메모리 파편화 현상을 막아주는 G1GC 가비지 컬렉터를 사용할 수도 있다.

익스큐터의 힙 메모리 크기를 늘리려면 `-XX:G1HeapRegionSize` 옵션을 설정해서 G1 영역의 크기를 증가시킬 수 있다.

## 직접적인 성능 향상 기법

### 병렬화

특정 스테이지의 처리 속도를 높이려면 병렬성을 높여야 한다.

`spark.default.parallelism` 과 `spark.sql.shuffle.partitions` 의 값을 클러스터 코어 수에 따라서 설정하자.

스테이지에서 처리해야 할 데이터량이 너무 많은 경우에는 익스큐터의 디스크 공간을 사용하지 않도록 하기 위해서 코어당 최소 2~3 개의 테스크를 할당하도록 해서 안전하게 처리하도록 하자.

### 향상된 필터링

스파크 잡에서 데이터 필터링 과정을 가장 먼저 수행하도록 하는 것은 자주 사용하는 성능 향상 기법이다.

상황에 따라서 데이터소스에 필터링을 위임해서 최종 결과와 무관한 데이터를 스파크에서 읽지 않고 작업을 진행하는 것도 있다.

그리고 파티셔닝과 버켓팅 기법을 활용해서 성능 향상에 도움을 줄 수 있다.

카디널리티 (Cardinality) 가 높으면, 즉 데이터 중복이 낮으면 파티셔닝에 불리한데 이 경우 bucket 을 사용하면 좋은 점이 많다. Bucketing 을 할 때 Bucket 의 숫자가 고정된다.

### 파티션 재분배와 병합

파티션 재분배 과정은 셔플을 동반한다. 하지반 클러스터 전체에 데이터가 균등하게 분배되므로 잡의 전체 실행 단계를 최저고하 할 수 있다.

일반적으로는 가능한 적은 양의 데이터를 셔플하는게 좋다.

그러므로 셔플 대신에 동일 노드의 파티션을 하나로 합치는 `coalesce` 메소드를 실행해서 DataFrame 이나 RDD 의 전체 파티션 수를 먼저  줄여야 한다. repartition 은 이보다 느리고 부하를 분산하기 위해서 네트워크로 데이터를 셔플링 한다.

repatition 을 사용하는 목적은 파티션의 숫자를 늘리거나 줄임을 통해서 데이터를 섞기 위한 것이다.

또 다른 방식인 repartitionAndSortWithinPartitions(partitioner) 이 있는데 이는 주어진 partitioner 에 대해서 repartition 을 해주고 레코드를 해당 키에 따라서 각각의 파티션을 정렬도 해준다.

### 사용자 정의 파티셔닝

사용자 정의 파티셔닝 기법은 사용자 정의 파티션 함수를 정의해서 DataFrame 보다 더 정밀한 수준으로 클러스터 전반에 있는 데이터를 제어할 수 있다.

이 방식은 아주 드물게 사용하지만 최적화 기법 중 하나다.

### 사용자 정의 함수

UDF 사용을 피하는 것도 아주 좋은 최적화 방법이다.

UDF 는 데이터를 JVM 객체로 변환하고 레코드당 여러 번 수행되므로 비효율 적이다.

따라서 사용자가 UDF 를 사용하는 것보다는 구조적 API 를 사용하도록 하는게 좋다.

### 임시 데이터 저장소 (캐싱)

애플리케이션에서 같은 데이터셋을 계속해서 재사용한다면 캐싱을 통해서 최적화할 수 있다.

캐싱은 익스큐터 전반에 만들어진 임시 저장소에 DataFrame 이나 RDD, 테이블 등을 보관해서 빠르게 저장하고 접근할 수 있다.

물론 캐싱을 저장할 땐 직렬화 가지고 올 땐 역직렬화를 사용하고 메모리 공간을 사용한다는 단점이 있다.

RDD 는 물리적 데이터 (bit 값) 을 캐시에 저장한다. 그리고 참조를 하면 캐싱된 데이터를 다시 읽고 적절한 데이터를 반환한다.

반면 구조적 API 캐싱은 물리적 실행 계획을 기 반으로 이뤄진다. 이는 객체 참조를 통해서 이뤄지는게 아니라 물리적 실행 계획을 키로 저장하고 처리 과정 동안 이 실행 계획을 참조한다.

간단하게 말하면 DataFrame 을 사용하는 쿼리를 수행하면 캐싱에서 읽어들인다 라고 생각하면 될 듯 싶다.

그리고 캐싱도 지연 처리라서 액션을 실행하는 시점에 데이터를 캐싱한다.

데이터를 캐싱할 때 사용할 수 있는 다양한 저장소 레벨이 있다.

- MEMORY_ONLY:
    - RDD 를 직렬화되지 않은 자바 객체로 저장한다. RDD 의 크기가 JVM 할당된 메모리보다 크면 메모리 크기를 벗어나는 파티션은 캐싱되지 않고 매번 즉시 재연산된다. (기본값이다.)
- MEMORY_AND_DISK:
    - RDD 를 직렬화되지 않은 자바 객체로 저장하는데 RDD 의 크기가 JVM 할당 메모리보다 크면 메모리 크기를 벗어나는 파티션을 디스크에 저장하고 디스크에서 읽어 들인다.
- MEMORY_ONLY_SER (자바와 스칼라에서만 가능)
    - RDD 를 직렬화한 자바 객체를 JVM 에 저장한다. (파티션당 하나의 바이트 배열) RDD 를 직렬화 하기 때문에 공간 효율성이 좋고 빠른 시리얼라이저를 사용하는 경우에 특히 좋다. 하지만 데이터를 읽을 떄 CPU 를 더 많이쓴다.
- MEMORY_AND_DISK_SER (자바와 스칼라에서만 가능)
    - MEMONY_ONLY_SER 과 유사하지만 메모리 크기를 벗어나는 파티션은 디스크에 기록하고 사용한다.
- DISK_ONLY
    - RDD 파티션을 디스크에만 저장한다.


### 조인

조인은 최적화를 위한 공통 영역이다.

브로드캐스트 조인 힌트를 사용하면 스파크가 쿼리 실행 계획을 세울 때 도와줄 수 있다.

조인을 최적화 하기 위해서는 조인 타입에 따른 특성과 동작 방식의 이해가 필수이다.

조인 순서를 변경하는 것은 INNER JOIN 을 사용해 필터링 하는 것과 동일한 효과를 누릴 수 있다.

equal join (= WHERE 절에서 등호 연산자를 사용해 조건 값이 정확하게 일치할 때 사용하는 조인) 은 최적화 하기 쉬우니 가장 먼저 사용하는게 좋다.

그리고 카테시안 조인이나 외부 조인은 피하는게 좋다.

그리고 조인을 할 때 테이블 통계를 수집해놓으면 스파크가 조인 타입을 결정하는데 유용하고 데이터를 적절하게 Bucketing 해놓으면 조인 수행시 거대한 셔플이 발생하지 않도록 미리 막을 수 있다.

### 집계

집계 전에 데이터를 필터링 해놓는게 좋다.

RDD 를 사용하면 집계 수행 방식을 정확하게 제어하고 코드의 성능과 안전성을 개선할 수 있다.

그리고 groupByKey 대신에 reduceByKey 를 사용하자.

### 브로드캐스트 변수

브로드 캐스트 변수는 모든 워커노드에 큰 규모의 입력 데이터를 효율적으로 제공할 때 사용하는 방법이며 모든 워커 노드에 저장해서 재전송 없이 Spark Action 에서 사용할 수 있도록 하는 것이다. (읽기 전용이다.) (통신 비용을 줄이기 위해서 사용한다.)

브로드캐스는 변수는 룩업 테이블이나 머신러닝 모델을 저장하는데 사용할 수 있다.

그리고 SparkContext 에서 브로드캐스트 변수를 생성하고 임의의 객체를 브로드캐스트 한 다음 테스크에서 참조하게 만들 수 있다.

## Appendix) 셔플이 도대체 뭔데?

- 데이터를 모우는 과정에서 발생함. 특정 파티션으로 모우는건가? ㅇㅇ
- 동일한 key 를 가진 튜플 데이터를 동일한 파티션에 두기 위해서 데이터의 위치를 재조정하는게 셔플이다.
- 셔플은 네트워크를 타고 전송을 한다. (그럴 수 밖에 없는데 1 core == 1 task == 1 partition 이다. 파티션으로 모우기 위해서는 다른 파티션에 있는 데이터를 전송해야하는데 이 과정에서 클러스터에 있는 서로 다른 컴퓨팅들에서 모아야 하므로.)

## Appendix) Spill 이 뭔데

- Executor 에서 감당할 수 없는 파티션 사이즈로 인해 최후로 디스크에 저장해놓고 메모리로 떙겨오고 하는 그런 과정
- Shuffle spill (memory) 는 디스크에서 메모리로 역직렬화하는 데이터 사이즈를 말한다.
- Shuffle spill (disk) 는 메모리에서 디스크로 저장할 때 직렬화하는 데이터 사이즈를 말한다.

## Appendix) Bucketing 이 뭔데

- 데이터 파티셔닝을 결정해줌으로써 셔플을 방지해주는 최적화 테크닉 중 하나다.
- 물론 초기에 bucketing 을 위해서 들여야 되는 시간이 필요하긴 하다.
- bucketing 은 특정 데이터 칼럼에 hash function 을 통해서 데이터를 정렬해서 파티션에 넣는 역할을 한다.
- bucketing 을 파티션안에 파티션이라고 볼 수 있다.

## Appendix) Executor 에 대해서

- Executor 는 Worker Node 에서 실행되는 Spark Application 의 JVM 프로세스다.
- `spark.executor.memory` 의 기본 값은 1GB 이다. (`spark.drive.memorty` 도 기본 값은 1GB 이다.)
- `spark.memory.fraction` 의 기본 값은 0.6 이다. Storage Region 을 위해서 사용한다. heap space 에서 300 MB * 1.5 정도 하면 된다.
- Spark 의 Task 는 Executor 의 다음 두 메모리 지역을 사용한다.
    - execution - join, shuffle, sorts, aggregation 연산을 위해서 사용하며 여기의 데이터는 short-lived 하다.
    - storage - partition 데이터의 캐시. 로 rdd.cache() 나 rdd.persist() 를 통해서 저장하는 데이터로 오래 사는 데이터다.
- partition 은 메모리나 디스크에서 존재할 수 있다.