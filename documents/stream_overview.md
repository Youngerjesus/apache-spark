# Spark Streaming Overview

아파치 스파크는 스트림 처리에 필요한 API 들을 오래전부터 지원했다.

- map, reduce() 와 같은 스트림 처리와, DStream 같은 저수준 API
- 요즘에는 Dataset 이나 DataFrame 에서 사용할 수 있는 구조적인 스트리밍 API 까지도 지원한다.

### 스트림 처리 (Stream Processing) 이란?

- 입력으로 들어오는 무한정의 데이터를 끊임없이 처리하는 걸 말한다.
- 이렇게 식ㄹ행하면서 다양한 결과들을 Key-Value 저장소 같은 외부 Sink 시스템에 저장할 수도 있다.
- 스트림 처리와 배치 처리를 비교하는 경우도 많다. 물론 현재는 같이 쓰는 경우도 많다.
    - 배치처리는 고정된 입력 데이터가 있고 처리한 결과는 한번만 주기적으로 만들어낸다.
    - 처리하는 연산은 비슷하다. 스트림 처리가 다만 효율적인 경우가 많다. 메모리에 현재까지 적용한 데이터들을 계속해서 가지고 있으니까 처음부터 작업하지 않아도 된다.
    - 스트림 처리와 배치 처리를 같이 사용하는 경우에는 스트림 데이터를 배치  처리로 만들어낸 데이터셋과 조인해서 작업하는 경우도 있다.

### 스트림 처리 사례

- 통보와 알림: 연속적인 데이터에서 패턴을 감지하고 이를 알려주도록 하는 것.
- 실시간 리포트: 실시간 대시보드를 만들 때 (전체 사용량, 시스템 부하, 실행 시간 등을 이용해서)
- 증분형 ETL: 배치에서 만든 데이터를 스트리밍을 통해서 계속해서 업데이트 해나가는 것.
    - 이때 중요한건 내고장성을 유지하면서 정확히 한 번 처리하는 것, 데이터 중복 저장이나 유실이 일어나면 안되는 것.
- 실시간 제공용 데이터 갱신:  다른 어플리케이션에 서비스를 제공하기 위해서 스트리밍 처리를 하는 경우
    - 예로 구글 애널리틱스 같은 경우는 실시간 방문자수를 추적해서 제공해준다.
- 실시간 의사결정: 연속적인 입력 데이터를 보고 비즈니스 로직에 따라 결정하는 것.
    - 예로 고객의 최근 이력을 기준으로 카드의 부정행위를 하는지 판단하는 것.
- 온라인 머신러닝: 여러 사용자의 실시간 데이터와 이력 데이터를 조합해서 모델을 학습하는 것.

### 스트림 처리의 장점

- 스트림 처리와 배치 처리를 비교해보면서 스트림 처리의 장점을 보자.
- 배치 처리는 일괄적으로 데이터를 모아서 처리하니까 처리량의 측면에서는 더 뛰어날 수 있다.
    - 그리고 더 직관적이고 간단하다.
- 스트림 처리는 다만 실시간으로 처리하니 대기시간이 배치처리보다 짧다.
- 그리고 이전의 연산 결과를 기억하고 재사용하니 더 효율적이다.

### 스트림 처리의 과제

- 스트림 처리의 대표적인 과제는 다음과 같다.
- 스트림 처리의 문제는 연속적으로 들어오는 데이터의 순서가 맞지 않을 수 있다.
    - 네트워크 이슈라던지, 스트림으로 보내는 데이터 소스의 이슈라던지.
- 입력값과 출력값에 트랜잭션을 보장해야하는 경우가 필요할 수 있다.
- 대규모 상태 정보를 저장하고 있어야 할 수 있다.
- 장애 상황에서도 정확히 한 번 처리할 수 있어야 한다.
- 신규 이벤트가 도착할 때 출력 싱크의 갱신 방법이 다양해야한다.
- 이벤트에 얼마나 빠르게 응답해야 하는가, 얼마나 많은 처리를 할 수 있어야 하는가

## 스트림 처리의 핵심 설계 개념

- 다양한 방법으로 위의 스트림 처리 과제를 해결할 수 있는데 이를 알아보기 전에 일반적인 스파크 스트리밍의 핵심 설계를 알아보자.

### 레코드 단위 처리와 선언형 API

- 스파크를 사용하지 않고 스트리밍 API 를 처리하는 방법은 이벤트만 어플리케이션에 전달하도록 하는 파이프라인 설계만 스트림 처리해서 해주는 방법이 있다.
- 이 경우를 레코드 단위 처리 (record-at-a-time) 이라고 부른다.
- 여기서의 문제는 어플리케이션 내부에서 중복 처리라던지, 내부 상태 저장이라던지 이런 것들을 신경써야 한다는 점이다.
- Spark Streaming 은 이렇게 하지 않고 선언형 (Declarative) API 를 제공해준다. 방법에 데해서 기술하는게 아니라. ”어떻게 처리되어 있을 것인지” 를 명시하도록 하는 방법이다.
- 그러므로 여기안에서는 중복 처리라던지 내고장성을 고려한 API 가 설계되었다.

### 이벤트 시간과 처리 시간

- 선언형 API 에서는 이벤트 시간을 사용할 지 처리 시간을 사용할 지 결정해야한다.
- 이벤트 시간은 스트리밍에 들어오는 레코드가 생성된 시간을 의미하고 타임 스탬프를 기반으로 한다.
- 처리 시간은 스트리밍 어플리케이션에 들어오는 시간을 의미한다.
- 그러므로 처리 시간을 기준으로 잡으면 순서가 뒤바뀌는 문제가 생길 수 있다는 것이다.
- 이벤트 시간을 사용할 땐 늦게 들어온 이벤트를 추적할 수 있는 상태를 가지고 있어야 한다. 즉 시스템이 해당 시간까지는 모두 입력데이터가 들어왔다고 가정할 수 있는 시기를 결정해야 한다.
- Spark 의 구조적 스트리밍 API 는 이벤트 시간을 기준으로 처리할 수 있도록 해놨다.

### 연속형 처리와 마이크로 배치 처리

- 연속형 처리 기반의 시스템은 레코드를 하나씩 읽고 처리하는 함수를 적용한 후 처리 결과를 전송하는 방식이다.
    - 즉 처리량은 낮을 수 있지만 응답속도는 높을 수 있다.
    - 레코드 단위로 전송하기 때문에 OS 호출이 많다.
- 마이크로 배치 는 입력 데이터를 작은 배치로 모우기 위해 대기하고 모아서 전송하는 개념이다. 따라서 처리량이 높을 수 있다.
- 마이크로 배치에서도 더 빠른 응답을 낼 수 있다. 결과물을 빠른 key-value 스토어에 저장하도록 하고 조회는 그 스토어에 요청하는 식으로.

## 스파크 스트리밍 API

- 스파크는 두 가지 스트리밍 API 를 제공한다.
    - 기존 저수준 API 인 DStream API 가 있고 이는 마이크로배치만 동작하며 이벤트 시간 처리 기능을 지원하지는 않는다.
    - 구조적 스트리밍 API 는 추상화를 해서 이벤트 시간 처리 연속형 처리 등을 지원한다.

### DStream API

- DStream API 는 선언형 API 로 스트리밍 시스템이 해결해야 하는 문제를 일부 해결해준다.
- 다만 이벤트 처리를 지원하지 않고 마이크로 배치만을 지원한다.
- 마지막으로 구조적 테이블 개념의 DataFrame 이나 Dataset 과 달리 자바나 파이썬 객체 함수에 의존적이라서 최적화 기법을 적용하기 힘들다.

### 구조적 스트리밍

- 스파크의 구조적 API 를 기반으로하는 스트리밍 API 이다.
- DStream 과는 다르게 최적화 기법을 제공해주고 자바, 스칼라, 파이썬, R, SQL 을 통한 구조적 처리를 할 수 있다.
- 구조적 스트리밍은 데이터가 도착할 때마다 자동으로 증분 형태의 연산 결과를 만들어내기 때문에 배치 처리와 스트리밍 처리 코드는 거의 같다.
- 구조적 스트리밍은 DStream API 에 사용성과 성능을 더한 것이라고 보면 된다.